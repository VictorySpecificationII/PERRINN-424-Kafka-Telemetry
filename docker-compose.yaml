version: '3.8'

networks:
  kafka-network:
    driver: bridge


# x-airflow-common:
#   &airflow-common
#   # In order to add custom dependencies or upgrade provider packages you can use your extended image.
#   # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
#   # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
#   image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.2}
#   # build: .
#   environment:
#     &airflow-common-env
#     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
#     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASS}@airflow_postgres/${AIRFLOW_DB_NAME}
#     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASS}@airflow_postgres/${AIRFLOW_DB_NAME}
#     AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
#     AIRFLOW__CORE__FERNET_KEY: ''
#     AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
#     AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
#     AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
#     # yamllint disable rule:line-length
#     # Use simple http server on scheduler for health checks
#     # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
#     # yamllint enable rule:line-length
#     AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
#     # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
#     # for other purpose (development, test and especially production usage) build/extend Airflow image.
#     _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
#     # The following line can be used to set a custom config file, stored in the local config folder
#     # If you want to use it, outcomment it and replace airflow.cfg with the name of your config file
#     # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
#   volumes:
#     - ./airflow-dags:/opt/airflow/dags
#     - airflow-logs:/opt/airflow/logs
#     - ./airflow-config.cfg:/opt/airflow/config
#     - airflow-plugins:/opt/airflow/plugins
#   user: "${AIRFLOW_UID:-50000}:0"
#   depends_on:
#     &airflow-common-depends-on
#     redis:
#       condition: service_healthy
#     airflow_postgres:
#       condition: service_healthy
#   networks:
#     - kafka-network


services:

  # Responsible for coordination and synchronization between distributed systems, such as Kafka and HBase.
  zookeeper:
    image: ubuntu/zookeeper:3.8-22.04_edge
    container_name: zookeeper
    networks:
      - kafka-network
  # Provides a web-based user interface for monitoring and managing Zookeeper clusters.
  zoonavigator:
    image: elkozmon/zoonavigator:latest
    container_name: zoonavigator
    networks:
      - kafka-network
    ports:
      - "${ZOONAVIGATOR_UI_PORT}:9000"
    environment:
      HTTP_PORT: 9000
      CONNECTION_LOCALZK_NAME: "Local ZooKeeper"
      CONNECTION_LOCALZK_CONN: "zookeeper:2181"
    depends_on:
      - zookeeper

 # A distributed streaming platform used for real-time data ingestion and processing.
  kafka:
    image: ubuntu/kafka:3.6-22.04_edge
    container_name: kafka
    networks:
      - kafka-network
    ports:
      - "${KAFKA_PORT}:9092"
    volumes:
      - ./kafka-server.properties:/etc/kafka/server.properties
    depends_on:
      - zookeeper

  # Offers a user interface for monitoring Kafka clusters and topics.
  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    networks:
      - kafka-network
    ports:
      - "${KAFKA_UI_PORT}:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: default
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      - kafka

  # # Offers real time stream processing, as opposed to spark which is used for batch processing
  # flink-jobmanager:
  #   build:
  #     context: .
  #     dockerfile: flinkDockerfile
  #   container_name: flink-jobmanager
  #   networks:
  #     - kafka-network
  #   ports:
  #     - "${FLINK_UI_PORT}:8081"
  #     - "${FLINK_RPC_PORT_JOBMANAGER}:6123"
  #   command: jobmanager
  #   environment:
  #     - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager

  # flink-taskmanager:
  #   build:
  #     context: .
  #     dockerfile: flinkDockerfile
  #   container_name: flink-taskmanager
  #   networks:
  #     - kafka-network
  #   command: taskmanager
  #   ports:
  #     - "${FLINK_RPC_PORT_TASKMANAGER}:6122"
  #     - "${FLINK_DATA_PORT_TASKMANAGER}:6121"
  #   environment:
  #     - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
  #   depends_on:
  #     - flink-jobmanager

  # Acts as the Spark master node, managing the allocation of resources and scheduling tasks across the Spark cluster.

  spark-master:
    image: bitnami/spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    networks:
      - kafka-network
    ports:
      - "${SPARK_UI_PORT}:8080"  # Spark UI
      - "${SPARK_MASTER_PORT}:7077"  # Spark Master
    volumes:
      - spark-logs:/opt/spark/logs

  spark-worker:
    image: bitnami/spark
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    networks:
      - kafka-network
    depends_on:
      - spark-master
    volumes:
      - spark-logs:/opt/spark/logs

  # A distributed NoSQL database that provides real-time read/write access to large datasets, often used for random, real-time access to big data.
#  hbase:
#    image: hyness/hbase-rest-standalone
#    container_name: hbase
#    networks:
#      - kafka-network
#    volumes:
#      - ./hbase-site.xml:/opt/hbase/conf/hbase-site.xml  # Mount custom configuration directory
#      - hbase_data:/data/hbase
#    ports:
     # - "2181:2181" #Zookeeper port, no need for it as we connect hbase to an external zookeeper
#      - "${HBASE_REST_API_PORT}:8080" # REST api port, can interact with hbase api through here
#      - "${HBASE_MASTER_PORT}:16000" # hbase master port
#      - "${HBASE_UI_PORT}:16010" # UI port
#      - "${HBASE_REGIONSERVER_PORT}:16020" # regionserver port, used by hbase region servers for handling read and write requests from clients.
#      - "${HBASE_REGIONSERVER_UI_PORT}:16030" # regionserver ui port
#    environment:
#      - HBASE_ZOOKEEPER_QUORUM=zookeeper
#      - HBASE_ZOOKEEPER_CLIENT_PORT=2181

  # cassandra:
  #   image: 'bitnami/cassandra:latest'
  #   container_name: cassandra
  #   hostname: cassandra
  #   ports:
  #     - '${CASSANDRA_PORT}:9042'
  #   #volumes:
  #     #- ./streaming-project:/home
  #   networks:
  #     - kafka-network

 # Apache Ozone section

  # datanode:
  #   container_name: ozone-datanode
  #   image: apache/ozone:1.4.0
  #   ports:
  #     - "${OZONE_DATANODE_PORT}:9864"
  #   command: ["ozone","datanode"]
  #   env_file:
  #     - ./ozone-config.env
  #   networks:
  #     - kafka-network
  #   volumes:
  #     - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  # om:
  #   image: apache/ozone:1.4.0
  #   container_name: ozone-om
  #   ports:
  #     - "${OZONE_OM_UI_PORT}:9874"
  #   environment:
  #     ENSURE_OM_INITIALIZED: /data/metadata/om/current/VERSION
  #     WAITFOR: scm:9876
  #   env_file:
  #     - ./ozone-config.env
  #   command: ["ozone","om"]
  #   networks:
  #     - kafka-network
  #   volumes:
  #     - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  # scm:
  #   image: apache/ozone:1.4.0
  #   container_name: ozone-scm
  #   ports:
  #     - "${OZONE_SCM_UI_PORT}:9876"
  #   env_file:
  #     - ./ozone-config.env
  #   environment:
  #     ENSURE_SCM_INITIALIZED: /data/metadata/scm/current/VERSION
  #   command: ["ozone","scm"]
  #   networks:
  #     - kafka-network
  #   volumes:
  #     - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  # recon:
  #   image: apache/ozone:1.4.0
  #   container_name: ozone-recon
  #   ports:
  #     - "${OZONE_RECON_UI_PORT}:9888"
  #   env_file:
  #     - ./ozone-config.env
  #   command: ["ozone","recon"]
  #   networks:
  #     - kafka-network
  #   volumes:
  #     - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  # s3g:
  #   image: apache/ozone:1.4.0
  #   container_name: ozone-s3g
  #   ports:
  #     - "${OZONE_S3G_S3_GATEWAY_ENDPOINT_PORT}:9878"
  #   env_file:
  #     - ./ozone-config.env
  #   command: ["ozone","s3g"]
  #   networks:
  #     - kafka-network
  #   volumes:
  #     - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml


  # airflow_postgres:
  #   image: postgres:13
  #   container_name: airflow_db
  #   environment:
  #     POSTGRES_USER: airflow
  #     POSTGRES_PASSWORD: airflow
  #     POSTGRES_DB: airflow
  #   volumes:
  #     - airflow_postgres_db_data:/var/lib/postgresql/data
  #   healthcheck:
  #     test: ["CMD", "pg_isready", "-U", "airflow"]
  #     interval: 10s
  #     retries: 5
  #     start_period: 5s
  #   restart: always
  #   networks:
  #     - kafka-network



  # redis:
  #   # Redis is limited to 7.2-bookworm due to licencing change
  #   # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
  #   image: redis:7.2-bookworm
  #   container_name: airflow_redis
  #   expose:
  #     - 6379
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 10s
  #     timeout: 30s
  #     retries: 50
  #     start_period: 30s
  #   restart: always
  #   networks:
  #     - kafka-network


  # airflow-webserver:
  #   <<: *airflow-common
  #   command: webserver
  #   container_name: airflow_webserver
  #   ports:
  #     - "${AIRFLOW_UI_PORT}:8080"
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   restart: always
  #   depends_on:
  #     <<: *airflow-common-depends-on
  #     airflow-init:
  #       condition: service_completed_successfully

  # airflow-scheduler:
  #   <<: *airflow-common
  #   command: scheduler
  #   container_name: airflow_scheduler
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   restart: always
  #   depends_on:
  #     <<: *airflow-common-depends-on
  #     airflow-init:
  #       condition: service_completed_successfully

  # airflow-worker:
  #   <<: *airflow-common
  #   command: celery worker
  #   container_name: airflow_worker
  #   healthcheck:
  #     # yamllint disable rule:line-length
  #     test:
  #       - "CMD-SHELL"
  #       - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   environment:
  #     <<: *airflow-common-env
  #     # Required to handle warm shutdown of the celery workers properly
  #     # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
  #     DUMB_INIT_SETSID: "0"
  #   restart: always
  #   depends_on:
  #     <<: *airflow-common-depends-on
  #     airflow-init:
  #       condition: service_completed_successfully

  # airflow-triggerer:
  #   <<: *airflow-common
  #   command: triggerer
  #   container_name: airflow_triggerer
  #   healthcheck:
  #     test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   restart: always
  #   depends_on:
  #     <<: *airflow-common-depends-on
  #     airflow-init:
  #       condition: service_completed_successfully

  # airflow-init:
  #   <<: *airflow-common
  #   entrypoint: /bin/bash
  #   container_name: airflow_init
  #   # yamllint disable rule:line-length
  #   command:
  #     - -c
  #     - |
  #       if [[ -z "${AIRFLOW_UID}" ]]; then
  #         echo
  #         echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
  #         echo "If you are on Linux, you SHOULD follow the instructions below to set "
  #         echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
  #         echo "For other operating systems you can get rid of the warning with manually created .env file:"
  #         echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
  #         echo
  #       fi
  #       one_meg=1048576
  #       mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
  #       cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
  #       disk_available=$$(df / | tail -1 | awk '{print $$4}')
  #       warning_resources="false"
  #       if (( mem_available < 4000 )) ; then
  #         echo
  #         echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
  #         echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
  #         echo
  #         warning_resources="true"
  #       fi
  #       if (( cpus_available < 2 )); then
  #         echo
  #         echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
  #         echo "At least 2 CPUs recommended. You have $${cpus_available}"
  #         echo
  #         warning_resources="true"
  #       fi
  #       if (( disk_available < one_meg * 10 )); then
  #         echo
  #         echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
  #         echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
  #         echo
  #         warning_resources="true"
  #       fi
  #       if [[ $${warning_resources} == "true" ]]; then
  #         echo
  #         echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
  #         echo "Please follow the instructions to increase amount of resources available:"
  #         echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
  #         echo
  #       fi
  #       mkdir -p /sources/logs /sources/dags /sources/plugins
  #       chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
  #       exec /entrypoint airflow version
  #   # yamllint enable rule:line-length
  #   environment:
  #     <<: *airflow-common-env
  #     _AIRFLOW_DB_MIGRATE: 'true'
  #     _AIRFLOW_WWW_USER_CREATE: 'true'
  #     _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USERNAME}
  #     _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
  #     _PIP_ADDITIONAL_REQUIREMENTS: ''
  #   user: "0:0"
  #   volumes:
  #     - ${AIRFLOW_PROJ_DIR:-.}:/sources

  # airflow-cli:
  #   <<: *airflow-common
  #   profiles:
  #     - debug
  #   environment:
  #     <<: *airflow-common-env
  #     CONNECTION_CHECK_MAX_COUNT: "0"
  #   # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
  #   command:
  #     - bash
  #     - -c
  #     - airflow
  #   container_name: airflow_cli

  # # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
  # # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # # See: https://docs.docker.com/compose/profiles/
  # flower:
  #   <<: *airflow-common
  #   command: celery flower
  #   profiles:
  #     - flower
  #   container_name: airflow_flower
  #   ports:
  #     - "5555:5555"
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   restart: always
  #   depends_on:
  #     <<: *airflow-common-depends-on
  #     airflow-init:
  #       condition: service_completed_successfully


  # Apache Hadoop Section (HDFS deprecated in favor of Ozone, todo - switch YARN cluster to Ozone at some point if needed)

  # NOTE: Hadoop itself includes HDFS (Hadoop Distributed File System) as one of its core components. 
  # NOTE: When you install Hadoop, you automatically get HDFS along with it. 
  # NOTE: HDFS is the primary storage layer in the Hadoop ecosystem, providing distributed storage for large-scale data processing. 
  # NOTE: So, you don't need to separately install HDFS when setting up a Hadoop cluster; it comes bundled with the Hadoop distribution.

  # Manages the filesystem namespace and metadata for HDFS (Hadoop Distributed File System). HDFS Component
#  namenode:
#    image: apache/hadoop:3
#    container_name: hadoop-namenode
#    hostname: namenode
#    command: ["hdfs", "namenode"]
#    ports:
#      - "${HDFS_NAMENODE_UI_PORT}:9870"
#    env_file:
#      - ./hadoop-config.env
#    environment:
#      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"


  # Stores actual data blocks and serves read and write requests from clients in HDFS. HDFS Component
#  datanode:
#    image: apache/hadoop:3
#    container_name: hadoop-datanode
#    command: ["hdfs", "datanode"]
#    env_file:
#      - ./hadoop-config.env

  # Manages resources in the YARN (Yet Another Resource Negotiator) cluster, allocating resources to applications.
#  resourcemanager:
#    image: apache/hadoop:3
#    container_name: hadoop-resourcemanager
#    hostname: resourcemanager
#    command: ["yarn", "resourcemanager"]
#    ports:
#       - "${HADOOP_RESOURCEMANAGER_UI_PORT}:8088"
#    env_file:
#      - ./hadoop-config.env
#    volumes:
#      - ./test.sh:/opt/test.sh

  # Manages resources and executes tasks on individual nodes in the YARN cluster.
#  nodemanager:
#    image: apache/hadoop:3
#    container_name: hadoop-nodemanager
#    command: ["yarn", "nodemanager"]
#    env_file:
#      - ./hadoop-config.env

  # The holy grail of monitoring - with this we can have out real time graphs
  # grafana:
  #   image: grafana/grafana:latest
  #   container_name: grafana
  #   networks:
  #     - kafka-network
  #   ports:
  #     - "${GRAFANA_UI_PORT}:3000"
  #   environment:
  #     - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER}
  #     - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
  #   volumes:
  #     - ./grafana/datasources:/etc/grafana/provisioning/datasources
  #     - ./grafana/dashboards:/etc/grafana/provisioning/dashboards

 # Machine Learning Tools

   # MLFlow Tracking Server to track experiments for ML
  # tracking_server:
  #   restart: always
  #   build:
  #     context: .
  #     dockerfile: mlflowDockerfile
  #   container_name: mlflow_server
  #   depends_on:
  #     - mlflow_db
  #     - s3
  #   ports:
  #     - "${MLFLOW_PORT}:5000"
  #   networks:
  #     - kafka-network
  #   environment:
  #     - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
  #     - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_ACCESS_KEY}
  #     - MLFLOW_S3_ENDPOINT_URL=http://s3:${MINIO_PORT}
  #     - MLFLOW_S3_IGNORE_TLS=true
  #   command: >
  #     mlflow server
  #     --backend-store-uri postgresql://${MLFLOW_PG_USER}:${MLFLOW_PG_PASSWORD}@mlflow_db:${MLFLOW_PG_PORT}/${MLFLOW_PG_DATABASE}
  #     --host 0.0.0.0
  #     --serve-artifacts
  #     --artifacts-destination s3://${MLFLOW_BUCKET_NAME}
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:${MLFLOW_PORT}/"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # mlflow_db:
  #   restart: always
  #   image: postgres
  #   container_name: mlflow_db
  #   expose:
  #     - "${MLFLOW_PG_PORT}"
  #   networks:
  #     - kafka-network
  #   environment:
  #     - POSTGRES_USER=${MLFLOW_PG_USER}
  #     - POSTGRES_PASSWORD=${MLFLOW_PG_PASSWORD}
  #     - POSTGRES_DATABASE=${MLFLOW_PG_DATABASE}
  #   volumes:
  #     - mlflow_db_data:/var/lib/postgresql/data/
  #   healthcheck:
  #     test: ["CMD", "pg_isready", "-p", "${MLFLOW_PG_PORT}", "-U", "${MLFLOW_PG_USER}"]
  #     interval: 5s
  #     timeout: 5s
  #     retries: 3

  # # Now we add the pgadmin4 service so we can monitor db contents should we need to
  # pgadmin4:
  #   image: dpage/pgadmin4:latest
  #   container_name: pgadmin4
  #   networks:
  #     - kafka-network
  #   environment:
  #     - PGADMIN_DEFAULT_EMAIL=${PGADMIN_DEFAULT_EMAIL}
  #     - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_DEFAULT_PASSWORD}
  #   ports:
  #     - "${PGADMIN4_PORT}:80"
  #   volumes:
  #     - ./pgadmin4_psql_servers.json:/pgadmin4/servers.json

  # # S3 storage for MLFlow
  # s3:
  #   restart: always
  #   image: minio/minio:latest
  #   container_name: mlflow_minio
  #   volumes:
  #     - minio_data:/data
  #   ports:
  #     - "${MINIO_PORT}:9000"
  #     - "${MINIO_CONSOLE_PORT}:9001"
  #   networks:
  #     - kafka-network
  #   environment:
  #     - MINIO_ROOT_USER=${MINIO_ROOT_USER}
  #     - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
  #     - MINIO_ADDRESS=${MINIO_ADDRESS}
  #     - MINIO_PORT=${MINIO_PORT}
  #     - MINIO_STORAGE_USE_HTTPS=${MINIO_STORAGE_USE_HTTPS}
  #     - MINIO_CONSOLE_ADDRESS=${MINIO_CONSOLE_ADDRESS}
  #   command: server --console-address ":${MINIO_CONSOLE_PORT}" /data
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
  #     interval: 30s
  #     timeout: 20s
  #     retries: 3

  # s3_mlflow_createbuckets:
  #   container_name: mlflow_create_bucket_job
  #   image: minio/mc
  #   depends_on:
  #     - s3
  #   networks:
  #     - kafka-network
  #   entrypoint: >
  #     /bin/sh -c "
  #     /usr/bin/mc alias set s3minio http://s3:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
  #     /usr/bin/mc mb s3minio/${MLFLOW_BUCKET_NAME};
  #     /usr/bin/mc policy set public s3minio/${MLFLOW_BUCKET_NAME};
  #     exit 0;
  #     "

  # s3_mlflow_createaccesskey:
  #   container_name: mlflow_create_accesskey_job
  #   image: minio/mc
  #   depends_on:
  #     - s3
  #   networks:
  #     - kafka-network
  #   entrypoint: >
  #     /bin/sh -c "
  #     /usr/bin/mc alias set s3minio http://s3:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
  #     /usr/bin/mc admin user svcacct add --access-key "${MINIO_ACCESS_KEY}" --secret-key "${MINIO_SECRET_ACCESS_KEY}" --name mlflow --description connection s3minio ${MINIO_ROOT_USER};
  #     exit 0;
  #     "
volumes:
  mlflow_db_data:
  minio_data:
  spark-logs:
  hbase_data:
  airflow-logs:
  airflow-plugins:
  airflow_postgres_db_data:
